{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Entire_Classical_Transformer_Structure\n",
        "\n",
        "With multipe helpers, I managed to complete Entire_Classical_Transformer_Structure. Now I'm very clear of it's build, and it's useful for further study of LLM like Bert, GPT 3, Llama 3, Qwen 2.5, Deepseek R1, Vit_base, Clip.\n",
        "\n",
        "I didn't try pretrain or inference because I only have limited GPU on google colab, and missed a lot techs like flash attention, page attention, KV cache (actually just import vLLM and it automatically has these methods. I learned concepts of these techs in Ju and Zhang's notes and tiny_llm notes, with Li Mu's reading paper videos)."
      ],
      "metadata": {
        "id": "cnrsygDPExT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "self-attention, numpy method:"
      ],
      "metadata": {
        "id": "c0NPsL_mmmoU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QjpD8mGRj-G"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_qkv(X, W_q, W_k, W_v):\n",
        "\tQ = X @ W_q\n",
        "\tK = X @ W_k\n",
        "\tV = X @ W_v\n",
        "\treturn Q, K, V\n",
        "\n",
        "def self_attention(Q, K, V):\n",
        "\tdim_K = K.shape[-1]\n",
        "\tscores = (Q @ K.T)/np.sqrt(dim_K)\n",
        "\tmax_scores = np.max(scores, axis = -1, keepdims = True)\n",
        "\tnew_scores = scores - max_scores\n",
        "\tattention_weights = np.exp(new_scores) / np.sum(np.exp(new_scores),axis = -1, keepdims = True)\n",
        "\tattention_output = attention_weights @ V\n",
        "\treturn attention_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "self-attention, pytorch method:"
      ],
      "metadata": {
        "id": "mLQK6tnntSo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, embed_dim):\n",
        "    super().__init__()\n",
        "    self.embedding_dim = embed_dim\n",
        "    self.get_q = nn.Linear(embed_dim, embed_dim, bias= False)\n",
        "    self.get_k = nn.Linear(embed_dim, embed_dim, bias= False)\n",
        "    self.get_v = nn.Linear(embed_dim, embed_dim, bias= False)\n",
        "\n",
        "  def forward(self, x, mask= False):  # x.shape is [B,s,e] B: batch size, s: sequence length, e: embedding size\n",
        "    Q = self.get_q(x)\n",
        "    K = self.get_k(x) # dim K is in each batch the embedding size = self.embedding_dim\n",
        "    V = self.get_v(x) # has shape [B,s,e]\n",
        "    attention_scores = torch.matmul(Q,K.transpose(-2,-1))/(self.embedding_dim**(-0.5))\n",
        "    max_scores = torch.max(attention_scores, dim =-1, keepdim= True)[0] # get max value of each row, not indices\n",
        "    new_scores = attention_scores - max_scores # torch will auto expand dim so every entry in a row - max value, and in softmax exp(negative value) won't be huge\n",
        "    attention_weights = torch.softmax(new_scores, dim =-1) # has shape [B,s,s]\n",
        "    attention_output = torch.matmul(attention_weights, V) # out put has same dim as x\n",
        "    return attention_output\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j3DzSNLbtbll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2) Create a dummy batch of embeddings ---\n",
        "B, T, C = 2, 5, 16           # batch size 2, sequence length 5, embed dim 16\n",
        "x = torch.randn(B, T, C)     # random input\n",
        "\n",
        "# --- 3) Instantiate and run ---\n",
        "attn_layer = SelfAttention(embed_dim=C)\n",
        "output = attn_layer(x)\n",
        "\n",
        "print(\"output shape:\", output.shape)   # → torch.Size([2, 5, 16])\n",
        "\n",
        "# --- 4) (Optional) Visualize one attention matrix ---\n",
        "print(\"attention weights for sample 0:\\n\", output[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdBxQowLtcEg",
        "outputId": "37a6eb6d-21e2-45df-8e7e-a8c5f54a8e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output shape: torch.Size([2, 5, 16])\n",
            "attention weights for sample 0:\n",
            " tensor([[ 0.4882, -0.1974,  0.4953,  0.0751,  0.3127, -0.0228, -0.0331,  0.3205,\n",
            "         -0.1828,  0.3713, -0.3189, -0.0096, -0.2000,  0.1807,  0.4232,  0.1530],\n",
            "        [ 0.5012,  0.0043, -0.5890, -0.1467, -0.4039, -0.6152, -0.8488,  0.2181,\n",
            "         -1.1083, -0.2581,  1.3502,  0.3465, -0.9484, -0.3295,  0.2146, -0.1683],\n",
            "        [ 0.6680, -0.3255,  0.2919,  0.0046,  0.2490, -0.2500, -0.0585,  0.1657,\n",
            "          0.0210,  0.3828, -0.3388,  0.0551, -0.1739, -0.0188,  0.2955,  0.2897],\n",
            "        [-0.0268,  0.1069,  1.0452,  0.2255,  0.4878,  0.6085, -0.0106,  0.7298,\n",
            "         -0.7010,  0.3331, -0.2881, -0.1790, -0.3185,  0.7287,  0.7232, -0.2406],\n",
            "        [-0.2968, -0.5228,  0.5184, -0.4663,  0.2590,  0.4869, -0.9638,  0.4480,\n",
            "         -0.3621,  0.0240, -0.0476,  0.0554, -1.2374,  0.4955, -0.1072, -0.5704]],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-head attention"
      ],
      "metadata": {
        "id": "Y3pZMzk6VMBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super().__init__()\n",
        "    assert d_model % num_heads == 0\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.d_k = d_model // num_heads\n",
        "\n",
        "    self.W_Q = nn.Linear(d_model, d_model) # in theory, Q is multiplied by num_heads small head W^{i}_Q. Here W_Q is concatenation of W^{i}_Q of all heads.\n",
        "    self.W_K = nn.Linear(d_model, d_model)\n",
        "    self.W_V = nn.Linear(d_model, d_model)\n",
        "    self.W_O = nn.Linear(d_model, d_model)\n",
        "\n",
        "  def forward(self, Q,K,V, mask = None): #each of Q,K,V has shape [B,s,d_model]:batch size, sequence length, embedding feature dimension size\n",
        "    batch_size = Q.size(0)\n",
        "    seq_len = Q.size(1)\n",
        "\n",
        "    Q = self.W_Q(Q).reshape(batch_size, seq_len, self.num_heads, self.d_k) #project Q onto all heads, and reshape to sepreate them\n",
        "    Q = Q.transpose(-2,-3) #now last two dims are (seq_len,d_k)\n",
        "    K = self.W_K(K).reshape(batchsize, seq_len, self.num_heads, self.d_k)\n",
        "    K = K.transpose(-2,-3)\n",
        "    V = self.W_V(V).reshape(batchsize, seq_len, self.num_heads, self.d_k)\n",
        "    V = V.transpose(-2,-3)\n",
        "\n",
        "    scores = torch.matmul(Q,K.transpose(-2,-1))/ math.sqrt(self.d_k)\n",
        "\n",
        "    # decide if use mask, if use, mask position that mask entry == 0, by adding -inf.\n",
        "    if mask is not None:\n",
        "      scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "    attention_weights = torch.softmax(scores, dim = -1) #softmax applied on each token's scores vector\n",
        "    attention_output = torch.matmul(attention_weights, V) # it's shape (batch_size, num_heads, seq_len, d_k)\n",
        "    attention_output.transpose(-2,-3)\n",
        "    attention_output.reshape(batch_size,seq_len,self.d_model) # it actually does concatenation of heads\n",
        "    output = self.W_O(attention_output) # 用来融合不同 head 的信息（这是论文中的 “output projection”）。\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JkX9QOmDVC4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.W_Q = nn.Linear(d_model, d_model)\n",
        "        self.W_K = nn.Linear(d_model, d_model)\n",
        "        self.W_V = nn.Linear(d_model, d_model)\n",
        "        self.W_O = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        batch_size = Q.size(0)\n",
        "\n",
        "        Q = self.W_Q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_K(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_V(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(attn, V)  # [batch, heads, seq_len, d_k]\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
        "        return self.W_O(out)"
      ],
      "metadata": {
        "id": "8VD26C20OHqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Complete transformer model\n",
        "\n",
        "\n",
        "##Transformer 模型（ 和论文一致）\n",
        "###sublayer：multi-head attention + position encoding + feedforward + residual + layernorm）。\n",
        "\n",
        "Comments: laynorm 在代码中我们实际使用 pre-norm， 即对每一个multi-head attention 和 position encoding + feedforward sublayer，先做layernorm 再做 sublayer，然后做 add residual。这样的好处：\n",
        "\n",
        "特性\tPost-Norm（论文原版）vs\tPre-Norm（后续改进）\n",
        "\n",
        "性能差异\t小规模时差别不大\t大规模模型普遍用 Pre-Norm\n",
        "\n",
        "换句话说：\n",
        "1.   归一化位置\t残差加法后\tvs 残差加法前\n",
        "2.   梯度流动\t梯度必须经过 LayerNorm，可能被削弱 vs\t梯度可以直接通过残差支路传播\n",
        "3.   训练稳定性\t在深层 Transformer（> 6 层）中容易梯度爆炸或消失\tvs 更稳定，支持更深层（甚至 100+ 层）\n",
        "\n",
        "总结：Pre-Norm 改进了 梯度流动路径，从而显著提升了深层 Transformer 的可训练性。"
      ],
      "metadata": {
        "id": "ZQhCXDWfJ1oX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Encoder-decoder architecture\n",
        "│\n",
        "├── Encoder (N 层)\n",
        "│   ├── Self-Attention\n",
        "│   ├── FeedForward\n",
        "│   └── Add & Norm\n",
        "│\n",
        "└── Decoder (N 层)\n",
        "    ├── Masked Self-Attention  ← causal\n",
        "    ├── Cross-Attention (对 Encoder 输出)\n",
        "    ├── FeedForward\n",
        "    └── Add & Norm'''"
      ],
      "metadata": {
        "id": "m_BrpSM4cvYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Architecture\n",
        "\n",
        "| 模块                   | 功能                                  |\n",
        "| -------------------- | ----------------------------------- |\n",
        "| `MultiHeadAttention` | 实现 Q-K-V 注意力机制                      |\n",
        "| `FeedForward`        | 两层前馈网络                              |\n",
        "| `LayerNorm`          | 层归一化（可训练 γ, β）                      |\n",
        "| `EncoderLayer`       | Self-Attn + FFN                     |\n",
        "| `DecoderLayer`       | Masked Self-Attn + Cross-Attn + FFN |\n",
        "| `Encoder/Decoder`    | 堆叠多层                                |\n",
        "| `Transformer`        | Encoder + Decoder + 输出层             |\n",
        "\n"
      ],
      "metadata": {
        "id": "mxrsHouCcoHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "# ========= 1️⃣ 位置编码 Positional Encoding =========\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch, seq_len, d_model]\n",
        "        seq_len = x.size(1)\n",
        "        return x + self.pe[:, :seq_len]\n",
        "\n",
        "# ========= 2️⃣ 多头注意力 Multi-Head Attention =========\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.W_Q = nn.Linear(d_model, d_model)\n",
        "        self.W_K = nn.Linear(d_model, d_model)\n",
        "        self.W_V = nn.Linear(d_model, d_model)\n",
        "        self.W_O = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        batch_size = Q.size(0)\n",
        "\n",
        "        Q = self.W_Q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_K(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_V(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(attn, V)  # [batch, heads, seq_len, d_k]\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
        "        return self.W_O(out)\n",
        "\n",
        "# ========= 3️⃣ 前馈网络 FeedForward =========\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# ========= 4️⃣ LayerNorm =========\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "\n",
        "# ========= 5️⃣ Encoder Layer =========\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.norm2 = LayerNorm(d_model)\n",
        "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Pre-Norm + 残差连接\n",
        "        x2 = self.attn(self.norm1(x), self.norm1(x), self.norm1(x), mask)\n",
        "        x = x + self.dropout(x2)\n",
        "        x2 = self.ffn(self.norm2(x))\n",
        "        x = x + self.dropout(x2)\n",
        "        return x\n",
        "\n",
        "# ========= 6️⃣ Decoder Layer =========\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.norm2 = LayerNorm(d_model)\n",
        "        self.norm3 = LayerNorm(d_model)\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, tgt_mask=None, memory_mask=None):\n",
        "        # Masked Self-Attention\n",
        "        x2 = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), tgt_mask)\n",
        "        x = x + self.dropout(x2)\n",
        "        # Cross Attention\n",
        "        x2 = self.cross_attn(self.norm2(x), enc_out, enc_out, memory_mask)\n",
        "        x = x + self.dropout(x2)\n",
        "        # Feed Forward\n",
        "        x2 = self.ffn(self.norm3(x))\n",
        "        x = x + self.dropout(x2)\n",
        "        return x\n",
        "\n",
        "# ========= 7️⃣ Encoder Stack =========\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "        for _ in range(num_layers)])\n",
        "        self.norm = LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, src, mask=None):\n",
        "        x = self.embed(src)\n",
        "        x = self.pos(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "# ========= 8️⃣ Decoder Stack =========\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "        for _ in range(num_layers)])\n",
        "        self.norm = LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, tgt, enc_out, tgt_mask=None, memory_mask=None):\n",
        "        x = self.embed(tgt)\n",
        "        x = self.pos(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_out, tgt_mask, memory_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "# ========= 9️⃣ 整体 Transformer =========\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab, tgt_vocab, d_model=512, num_layers=6,\n",
        "                 num_heads=8, d_ff=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, d_model, num_layers, num_heads, d_ff, dropout)\n",
        "        self.decoder = Decoder(tgt_vocab, d_model, num_layers, num_heads, d_ff, dropout)\n",
        "        self.output = nn.Linear(d_model, tgt_vocab)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None):\n",
        "        enc_out = self.encoder(src, src_mask)\n",
        "        dec_out = self.decoder(tgt, enc_out, tgt_mask, memory_mask)\n",
        "        return self.output(dec_out)"
      ],
      "metadata": {
        "id": "hFbaSol9J-HN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simple test\n",
        "src = torch.randint(0, 100, (2, 10))  # batch=2, seq_len=10\n",
        "tgt = torch.randint(0, 100, (2, 9))\n",
        "\n",
        "model = Transformer(src_vocab=100, tgt_vocab=100)\n",
        "out = model(src, tgt)\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "id": "fkTni8EydmRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "训练循环（training loop）。\n",
        "\n",
        "我们使用经典设置：\n",
        "\n",
        "任务：序列到序列（seq2seq），比如机器翻译（n to m）， 文本分类（n to 1）；\n",
        "\n",
        "损失函数：nn.CrossEntropyLoss；\n",
        "\n",
        "优化器：torch.optim.Adam；\n",
        "\n",
        "mask：包含 padding mask 和 causal mask（上三角 mask）。"
      ],
      "metadata": {
        "id": "nctAK-m2dJQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 准备工具函数\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ===== 生成 mask =====\n",
        "\n",
        "def generate_square_subsequent_mask(sz: int):\n",
        "    \"\"\"生成上三角 causal mask（未来 token 置 -inf）\"\"\"\n",
        "    mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
        "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
        "    return mask  # [seq_len, seq_len]\n",
        "\n",
        "def create_padding_mask(seq, pad_idx=0):\n",
        "    \"\"\"生成 padding mask\"\"\"\n",
        "    return (seq != pad_idx).unsqueeze(1).unsqueeze(2)  # [batch,1,1,seq_len]"
      ],
      "metadata": {
        "id": "Wbw-u3dweG12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 准备模型和优化器\n",
        "\n",
        "from torch import nn, optim\n",
        "\n",
        "SRC_VOCAB = 100\n",
        "TGT_VOCAB = 100\n",
        "PAD_IDX = 0\n",
        "\n",
        "model = Transformer(src_vocab=SRC_VOCAB, tgt_vocab=TGT_VOCAB, d_model=512)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n"
      ],
      "metadata": {
        "id": "b1amq5FRePoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 构造一批数据 (data example)\n",
        "\n",
        "batch_size = 2\n",
        "src_seq_len = 10\n",
        "tgt_seq_len = 9\n",
        "\n",
        "src = torch.randint(1, SRC_VOCAB, (batch_size, src_seq_len))\n",
        "tgt = torch.randint(1, TGT_VOCAB, (batch_size, tgt_seq_len))\n",
        "tgt_y = torch.randint(1, TGT_VOCAB, (batch_size, tgt_seq_len))  # 预测目标\n"
      ],
      "metadata": {
        "id": "aOYCPgSleljd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 生成 mask\n",
        "\n",
        "# 1️⃣ 源序列 padding mask\n",
        "src_mask = create_padding_mask(src, pad_idx=PAD_IDX)\n",
        "\n",
        "# 2️⃣ 目标序列 causal mask（防止看未来）\n",
        "tgt_seq_len = tgt.size(1)\n",
        "tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "\n",
        "# 3️⃣ 目标 padding mask（防止计算无效 token）\n",
        "tgt_padding_mask = create_padding_mask(tgt, pad_idx=PAD_IDX)\n"
      ],
      "metadata": {
        "id": "SuF53UwBev6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 训练循环（一个 epoch 示例）\n",
        "\n",
        "model.train()\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# 前向传播\n",
        "out = model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
        "\n",
        "# 输出维度: [batch, tgt_len, vocab]\n",
        "# 调整为 CrossEntropyLoss 需要的 [batch*tgt_len, vocab]\n",
        "logits = out.view(-1, TGT_VOCAB)\n",
        "targets = tgt_y.view(-1)\n",
        "\n",
        "loss = criterion(logits, targets)\n",
        "loss.backward()\n",
        "\n",
        "# 梯度裁剪（防止梯度爆炸）\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "optimizer.step()\n",
        "\n",
        "print(f\"Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "GFfdJ6ROev9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. 完整最简训练循环模板\n",
        "\n",
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    src_mask = create_padding_mask(src)\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt.size(1))\n",
        "\n",
        "    out = model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
        "    logits = out.view(-1, TGT_VOCAB)\n",
        "    targets = tgt_y.view(-1)\n",
        "\n",
        "    loss = criterion(logits, targets)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss = {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "DVfFf2eYewDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##总结：训练流程\n",
        "\n",
        "| 步骤          | 内容                                       |\n",
        "| ----------- | ---------------------------------------- |\n",
        "| 1️⃣ 数据输入    | `src`, `tgt`, `tgt_y`                    |\n",
        "| 2️⃣ Mask 构造 | `src_mask` (padding)，`tgt_mask` (causal) |\n",
        "| 3️⃣ 前向传播    | `model(src, tgt, ...)`                   |\n",
        "| 4️⃣ 计算损失    | `CrossEntropyLoss(ignore_index=PAD_IDX)` |\n",
        "| 5️⃣ 反向传播    | `.backward()`                            |\n",
        "| 6️⃣ 优化参数    | `optimizer.step()`                       |\n",
        "| 7️⃣ 重复训练    | epoch 循环                                 |\n",
        "\n",
        "Comments：最终输出：\n",
        "每个目标 token 的预测概率分布 [batch, tgt_len, vocab_size]，\n",
        "训练时用 CrossEntropyLoss 对比真实目标 tgt_y。"
      ],
      "metadata": {
        "id": "3U0faG44f4zG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "autoregressive inference (greedy and sampling) using the Transformer you wrote earlier.\n",
        "\n",
        "Key ideas:\n",
        "\n",
        "Encode the source once (enc_out).\n",
        "\n",
        "Start tgt with a start token (BOS).\n",
        "\n",
        "Repeatedly decode using current tgt to get logits for the next token, pick next token (greedy / sample), append, stop on EOS or max length.\n",
        "\n",
        "Use causal mask for target self-attention so each step cannot attend to future tokens.\n",
        "\n",
        "Provide memory_mask (padding mask) for encoder outputs so cross-attention ignores padded positions.\n",
        "\n",
        "Use torch.no_grad() and model.eval()."
      ],
      "metadata": {
        "id": "WUfY1Wo-gd-0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wdpY3lhvewRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KIGxNXl7ewY-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}